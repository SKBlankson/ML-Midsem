{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d0c32f-9c6a-45af-97ef-f7e681c2bcd3",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e59c1e-25b2-49d3-b90e-bda1490cc801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59b2d7-64c9-4d3d-a417-4bd4818fe168",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44050319-8525-4e24-a021-8970b361611e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...</td>\n",
       "      <td>[Reporting, process, \\n\\n, by, Gilberto, Gambo...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Design Thinking for Innovation\\n\\nSindy Samaca...</td>\n",
       "      <td>[Design, Thinking, for, Innovation, \\n\\n, Sind...</td>\n",
       "      <td>[True, True, True, False, False, True, False, ...</td>\n",
       "      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>Assignment:  Visualization Reflection  Submitt...</td>\n",
       "      <td>[Assignment, :,   , Visualization,  , Reflecti...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                          full_text  \\\n",
       "0         7  Design Thinking for innovation reflexion-Avril...   \n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
       "2        16  Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...   \n",
       "3        20  Design Thinking for Innovation\\n\\nSindy Samaca...   \n",
       "4        56  Assignment:  Visualization Reflection  Submitt...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Design, Thinking, for, innovation, reflexion,...   \n",
       "1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
       "2  [Reporting, process, \\n\\n, by, Gilberto, Gambo...   \n",
       "3  [Design, Thinking, for, Innovation, \\n\\n, Sind...   \n",
       "4  [Assignment, :,   , Visualization,  , Reflecti...   \n",
       "\n",
       "                                 trailing_whitespace  \\\n",
       "0  [True, True, True, True, False, False, True, F...   \n",
       "1  [True, False, False, True, True, False, False,...   \n",
       "2  [True, False, False, True, True, False, False,...   \n",
       "3  [True, True, True, False, False, True, False, ...   \n",
       "4  [False, False, False, False, False, False, Fal...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
       "1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...  \n",
       "2  [O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...  \n",
       "3  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_data = pd.read_json('train.json')\n",
    "kaggle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220cdeb8-42ae-4c69-bae6-156caad29060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "681"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the first 90% of the data as the train set\n",
    "test_size = round(len(kaggle_data)*0.1) \n",
    "train_size = len(kaggle_data) - test_size\n",
    "\n",
    "# train_set, test_set = train_test_split(kaggle_data, test_size=len(kaggle_data)-train_size, random_state = False)\n",
    "train_set = kaggle_data.iloc[:train_size].copy()\n",
    "test_set = kaggle_data.iloc[train_size:].copy()\n",
    "\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9b4289-19f7-452e-8457-637ab6386a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = kaggle_data['full_text'].tolist()\n",
    "tokens = kaggle_data['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5c3d8-0215-4ef1-9670-1f26939635b4",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daac06fa-77a6-4ff1-87b0-055bff8332ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = {}\n",
    "with open('glove_model/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "063023e2-f619-4aca-b15d-c6b4418f86ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "      <th>embedded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "      <td>[[-1.1124498523899875, -0.07852007203381069, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "      <td>[[0.3832847211151008, -1.1155913441374923, 0.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                          full_text  \\\n",
       "0         7  Design Thinking for innovation reflexion-Avril...   \n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Design, Thinking, for, innovation, reflexion,...   \n",
       "1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
       "\n",
       "                                 trailing_whitespace  \\\n",
       "0  [True, True, True, True, False, False, True, F...   \n",
       "1  [True, False, False, True, True, False, False,...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...   \n",
       "1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...   \n",
       "\n",
       "                                       embedded_text  \n",
       "0  [[-1.1124498523899875, -0.07852007203381069, 1...  \n",
       "1  [[0.3832847211151008, -1.1155913441374923, 0.3...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_dataset = []\n",
    "for row in kaggle_data[\"tokens\"]:\n",
    "    embedded_essay = []\n",
    "    for token in row:\n",
    "        if token in glove_embeddings:\n",
    "            embedded_essay.append(glove_embeddings[token])\n",
    "        else:\n",
    "            # Handle out-of-vocabulary tokens\n",
    "            # For example, initialize their embedding randomly or use a special token\n",
    "            embedded_essay.append(np.random.randn(200))  # 100 is the embedding dimension\n",
    "    \n",
    "    embedded_dataset.append(embedded_essay)\n",
    "\n",
    "# Assign the embedded essays to each document\n",
    "kaggle_data[\"embedded_text\"] = embedded_dataset\n",
    "kaggle_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9309f5-a874-4d19-81d6-33c754b77433",
   "metadata": {},
   "source": [
    "### Sanity check to ensure that the tokens are the same length as the embedded representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdfca08-0c95-40d4-aefb-c28e7bffa9fc",
   "metadata": {},
   "source": [
    "Here, we check to make sure that number of tokens and labels for each document is equal to the number of embedded tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec0b16f8-64d6-4608-ab03-6293f91a059a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753\n",
      "753\n",
      "753\n"
     ]
    }
   ],
   "source": [
    "# Number of tokens in the first document\n",
    "print(len(kaggle_data.iloc[0]['tokens']))\n",
    "\n",
    "# Number of embeded tokens in the first document\n",
    "print(len(kaggle_data.iloc[0]['embedded_text']))\n",
    "\n",
    "# Number of labels in the first document\n",
    "print(len(kaggle_data.iloc[0]['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f184c3-51ae-4e15-a3a3-0125df1c646b",
   "metadata": {},
   "source": [
    "# Define The Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df95ec0-0d8d-4f14-8a52-3fbf18fc976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleData(d2l.DataModule):\n",
    "    \"\"\" Data for linear regressions \"\"\"\n",
    "    def __init__(self, features, targets, num_train, num_val, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.X = features\n",
    "        self.y = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7980db53-d4dc-4948-b425-de93c875441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(KaggleData)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3a73722-4c68-4979-921f-44eb065f1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNNScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.num_hiddens *= 2  # The output dimension will be doubled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4486665-682f-49e7-aecf-51b0715b8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(BiRNNScratch)\n",
    "def forward(self, inputs, Hs=None):\n",
    "    f_H, b_H = Hs if Hs is not None else (None, None)\n",
    "    f_outputs, f_H = self.f_rnn(inputs, f_H)\n",
    "    b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\n",
    "    outputs = [torch.cat((f, b), -1) for f, b in zip(\n",
    "        f_outputs, reversed(b_outputs))]\n",
    "    return outputs, (f_H, b_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c597e697-1231-4e9f-9c75-35f3300dfe8d",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9089f5e5-39d3-42c5-b893-c93d2ad769ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "\n",
    "# class BiLSTMNER(nn.Module):\n",
    "#     def __init__(self, num_embeddings, embedding_dim, num_classes, hidden_size):\n",
    "#         super(BiLSTMNER, self).__init__()\n",
    "#         self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_size, bidirectional=True)\n",
    "#         self.fc = nn.Linear(2*hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         embedded = self.embedding(x)\n",
    "#         embedded = embedded.permute(1, 0, 2)  # LSTM expects (seq_len, batch, features)\n",
    "#         lstm_out, _ = self.lstm(embedded)\n",
    "#         lstm_out = lstm_out.permute(1, 0, 2)  # Back to (batch, seq_len, features)\n",
    "#         logits = self.fc(lstm_out)\n",
    "#         return logits\n",
    "\n",
    "# # Example usage:\n",
    "# num_embeddings = 10000  # Example number of embeddings\n",
    "# embedding_dim = 100\n",
    "# num_classes = 5  # Number of classes (e.g., B-PER, I-PER, B-LOC, I-LOC, O)\n",
    "# hidden_size = 128\n",
    "# model = BiLSTMNER(num_embeddings, embedding_dim, num_classes, hidden_size)\n",
    "\n",
    "# # Define loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training loop (assuming you have a DataLoader named train_loader)\n",
    "# for epoch in range(2):\n",
    "#     for inputs, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs.view(-1, num_classes), labels.view(-1))\n",
    "#         loss.backward()\n",
    "#         optimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04d1e9ff-4835-4440-962e-02cd2e8bd689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# rnn = nn.LSTM(10, 20, 2)\n",
    "# input = torch.randn(5, 3, 10)\n",
    "# h0 = torch.randn(2, 3, 20)\n",
    "# c0 = torch.randn(2, 3, 20)\n",
    "# output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92270860-5efd-4ac0-8e48-a0287e5fbb22",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73440a8f-481e-4692-a932-35d17150dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3b77a-662f-4532-8465-60747cb2a9c3",
   "metadata": {},
   "source": [
    "### Batching the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea9de5-d1aa-4e12-a4f6-b1df19c5a5db",
   "metadata": {},
   "source": [
    "We then batch the training data set into batches of size 64. Batching data allows the torch framework \n",
    "to take advantage of  parralelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e82892da-f931-4857-b06c-c54225d3b2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x463f3f4d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "batch_dataloader = DataLoader(kaggle_data, batch_size=batch_size, shuffle=True)\n",
    "batch_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995c3a7-2959-452d-af04-813173d17c21",
   "metadata": {},
   "source": [
    "### Instantiate our BiLSTM and its variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecfb3b0e-1f4b-4a19-9544-82a5c9af509f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4226648142.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    input =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "input_size = 200# Set the number of features for the input\n",
    "hidden_size = 200 \n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "input = \n",
    "\n",
    "model = torch.nn.LSTM(self, input_size = input_size, \n",
    "                     hidden_size = hidden_size, \n",
    "                     num_layers= num_layers, \n",
    "                     bidirectional=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
